---
layout: about
title: me
permalink: /
description: 

profile:
  align: right
  image: zxxie_profile.jpeg
  address: 


news: false  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page




---


## **Zhixian Xie**

<p style="margin-bottom:1.3cm; margin-left: 0.5cm"> </p>


I am PhD student at <a href="https://asu-iris.github.io/">Intelligent Robotics and Interactive Systems (IRIS) Lab</a> at Arizona State University. 

In 2022, I received my B.Eng degree in automation at Shanghai Jiao Tong University, Shanghai, China. I joined IRIS lab for PhD study in 2024.1,  working with <a href="https://wanxinjin.github.io/">Dr. Wanxin Jin</a>. 

<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>



<center>
    <a href = "mailto:zxxie@asu.edu" target="_blank"> 
    <img src="assets/img/platform_icon/email.gif" width="35" target="_blank"> </a>   &nbsp;&nbsp;&nbsp;
<a href = "https://scholar.google.com/citations?user=Q-IQZRsAAAAJ&hl=en&authuser=1" target="_blank"> 
    <img src="assets/img/platform_icon/scholar.png" width="35" target="_blank"></a>   &nbsp;&nbsp;&nbsp;
<a href = "https://github.com/Zhi-Xian-Xie" target="_blank">
    <img src="assets/img/platform_icon/github.gif" width="35" target="_blank"></a> &nbsp;&nbsp;&nbsp;
<a href = "https://www.linkedin.com/in/zhixian-xie-75b599355" target="_blank">
    <img src="assets/img/platform_icon/linkedin.png" width="35" target="_blank"></a>  &nbsp;&nbsp;&nbsp;
<!-- <a href = "https://www.youtube.com/channel/UCkMgzXIhi3BmWP7tAdeyoaA" target="_blank">
    <img src="assets/img/platform_icon/youtube.gif" width="35" target="_blank"></a>  &nbsp;&nbsp;&nbsp; -->
</center>
<br />
<hr />
<br />
<center>
    <h4><strong>My Research</strong></h4>
</center>

<p style="margin-left: 1.5cm"> </p>
My research focuses on:

- **Robot Learning from Human** I develop algorithms that enable robots to learn efficiently from human dataâ€”such as preferences, corrections, and demonstrations. I am interested in imitation learning and interactive learning paradigms to allow robots to generalize from limited supervision and align their behavior with human intent, facilitating natural and adaptive human-robot collaboration.

- **Dexterous Manipulation** I design model-based and model-free methods for dynamic, contact-rich manipulation with high-DOF robotic hands. My research integrates control theory and learning to enable robust, generalizable skills such as in-hand reorientation and non-prehensile manipulation, advancing autonomous dexterity in real-world settings.
<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>


-----

<center>
    <h4><strong>Publication & Preprints</strong></h4>
</center>


<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>
<img src="collections/research/human/safe_alignment.gif"  width="160"  align="left" hspace="30" vspace=0 />

**Safe MPC Alignment with Human Directional Feedback** <br />
<b>Zhixian Xie</b>, Wenlong Zhang, Yi Ren, Zhaoran Wang, George. J. Pappas and Wanxin Jin<br />
*Preprint, submitted to IEEE Transactions on Robotics (T-RO)* <br />
[[PDF]](https://arxiv.org/abs/2407.04216){:target="_blank"}/ 
[[Code]](https://github.com/asu-iris/Safe-MPC-Alignment){:target="_blank"}/
[[Video]](https://youtu.be/QOODShHLQJE){:target="_blank"}/
[[Webpage]](https://zhi-xian-xie.github.io/safe_alignment_site/){:target="_blank"}

<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>
<img src="collections/research/VLM/vlm.gif"  width="160"  align="left" hspace="30" vspace="20" />

**Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos** <br />
Harsh Mahesheka, <b>Zhixian Xie</b>, Zhaoran Wang, Wanxin Jin <br />
*arXiv preprint, 2024* <br />
[[PDF]](https://arxiv.org/abs/2410.09286){:target="_blank"}/ 
[[Video]](https://www.youtube.com/watch?v=CzlyYLu4mLQ){:target="_blank"}/
[[Webpage]](https://harshmahesheka.github.io/vid-to-reward/){:target="_blank"}

<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>
<img src="collections/research/human/cut_robust.png"  width="160"  align="left" hspace="30" vspace="20" />

**Robust Reward Alignment via Hypothesis Space Batch Cutting** <br />
<b>Zhixian Xie</b>, Haode Zhang, Yizhe Feng, Wanxin Jin <br />
*Accepted by ICML 2025* <br />
[[PDF]](https://arxiv.org/abs/2410.09286){:target="_blank"}/ 
[[Code]](https://github.com/asu-iris/HSBC-Robust-Learning){:target="_blank"}/
[[Webpage]](https://zhi-xian-xie.github.io/HSBC-page/){:target="_blank"}
<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>

<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>
<img src="collections/research/manipulation/comp_free_gif.gif"  width="160"  align="left" hspace="30" vspace="20" />

**On-Palm Dexterity: Dynamic Reorientation of Objects
via Emergent Flipping and Sliding** <br />
<b>Zhixian Xie</b>, Wen Yang, Wanxin Jin <br />
*ICRA 2025 Workshop on Learning Meets Model-Based Methods for Contact-Rich Manipulation* <br />
[[PDF]](https://contact-rich.github.io/assets/pdf/papers/37_On_Palm_Dexterity_Dynamic.pdf){:target="_blank"}/ 
[[Webpage]](https://zhi-xian-xie.github.io/comp_free_ctrl_site/){:target="_blank"}
<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>


<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>
<img src="collections/research/manipulation/twintrack.gif"  width="160"  align="left" hspace="30" vspace="20" />

**TwinTrack: Bridging Vision and Contact Physics for Real-Time Tracking of Unknown Dynamic Objects** <br />
Wen Yang, <b>Zhixian Xie</b>, Xuechao Zhang, Heni Ben Amor, Shan Lin, Wanxin Jin <br />
*arXiv preprint, 2025* <br />
[[PDF]](https://arxiv.org/abs/2505.22882){:target="_blank"}/ 
[[Webpage]](https://irislab.tech/TwinTrack-webpage/){:target="_blank"}/
[[Video]](https://www.youtube.com/watch?v=UGn-kZzQsEg){:target="_blank"}
<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>

